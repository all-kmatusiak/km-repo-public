{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark basics\n",
    "\n",
    "This tutorial shows how to work with Spark in the local mode (spark-submit --master local).<br/>\n",
    "It uses your machine resources, so it will be significantly slower than using a dataproc cluster (see `SparkCluster.ipynb`).<br/>\n",
    "We only recommend using Spark this way if the job is very small and doesn't require a lot of resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make your Spark available in Notebook (required every time after kernel reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use your Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup configuration variables\n",
    "!IMPORTANT! *temp_bucket_name* bucket has to be created before running the calculations below. It needs to be in **the same** GC location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = 'km-notebook-app'\n",
    "project_id = 'sc-10067-search-analytics-prod'\n",
    "temp_bucket_name = 'km-bucket-050701'\n",
    "bq_table_in = 'sc-9366-nga-prod.search_logs.search_logs'\n",
    "bq_table_out = 'sc-10067-search-analytics-prod.km_workspace.listing_spark'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not created the bucket yet, you can use examples from **`/xwing-helpers-repository/tutorials/storage/`** folder or run following code snipper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "print(\"Client created using default project: {}\".format(client.project))\n",
    "\n",
    "# Replace the string below with location code where the new bucket will be created\n",
    "location_code = \"europe-west1\" \n",
    "temp_bucket_name = \"km-bucket-test\"\n",
    "\n",
    "def create_bucket(bucket_name, location_code):\n",
    "    \"\"\"Creates a new bucket.\"\"\"\n",
    "    # bucket_name = \"your-new-bucket-name\"\n",
    "    # location_code = \"gcs region name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    bucket = storage.Bucket(storage_client, bucket_name)\n",
    "    bucket.storage_class = \"REGIONAL\"\n",
    "    bucket.create(location = location_code)\n",
    "\n",
    "    print(\"Bucket {} created\".format(bucket.name))\n",
    "    \n",
    "create_bucket(temp_bucket_name, location_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SparkSession\n",
    "#### import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### configuration and creation of spark session:\n",
    "* .config('temporaryGcsBucket', temp_bucket_name) - registration of temp bucket for future big query writes\n",
    "* ._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem') - setting GCS as default file store for spark\n",
    "* ._jsc.hadoopConfiguration().set('fs.gs.project.id', project_id) - setting project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(app_name) \\\n",
    "  .config('spark.datasource.bigquery.temporaryGcsBucket', temp_bucket_name) \\\n",
    "  .master(\"local\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.gs.project.id', project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data from Big Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimate cost of fething data from bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT column1, column2, columnN FROM `{}` WHERE DATE(_PARTITIONTIME) = \"YYYY-MM-DD\"\n",
    "\"\"\".format(bq_table_in)\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.dry_run = True\n",
    "job_config.use_query_cache = False\n",
    "query_job = bigquery.Client().query(\n",
    "  (query),\n",
    "    location=\"EU\",\n",
    "    job_config=job_config\n",
    ")\n",
    "data_amt = query_job.total_bytes_processed\n",
    "print(\"{} GB will be processed which is roughly {} USD cost\".format(round(data_amt / 1024**3,2), round(data_amt / 1024**4 * 5,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT right(service_id, 2) AS marketplace,\n",
    "offer.position_in_context as position,\n",
    "count(offer.clicks[SAFE_OFFSET(0)]) as clicks\n",
    "FROM `sc-9366-nga-prod.search_logs.search_logs`, unnest(offer_listing.offers_with_ads) AS offer\n",
    "WHERE TIMESTAMP_TRUNC(_PARTITIONTIME, DAY) = TIMESTAMP(\"2024-09-15\")\n",
    "and offer_listing.page = 1\n",
    "and search_phrase.raw is not null\n",
    "and offer_listing.sorting = \"RELEVANCE_DESC\"\n",
    "and tag.raw = 'ProductListing'\n",
    "and offer.position_in_context is not null\n",
    "and offer.clicks is not null\n",
    "group by marketplace, position\n",
    "order by marketplace, position\n",
    "\"\"\".format(bq_table_in)\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.dry_run = True\n",
    "job_config.use_query_cache = False\n",
    "query_job = bigquery.Client().query(\n",
    "  (query),\n",
    "    location=\"EU\",\n",
    "    job_config=job_config\n",
    ")\n",
    "data_amt = query_job.total_bytes_processed\n",
    "print(\"{} GB will be processed which is roughly {} USD cost\".format(round(data_amt / 1024**3,2), round(data_amt / 1024**4 * 5,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data using Storage Read API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df = spark.read.format('bigquery') \\\n",
    "#   .option('table', bq_table_in) \\\n",
    "#   .option('filter', \"date < '1959-08-21'\") \\\n",
    "#   .load()\n",
    "\n",
    "spark_df = spark.read.format('bigquery') \\\n",
    "  .option('table', bq_table_in) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run calculation on read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = spark_df.withColumn('year', year(col('date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "exploded_df = spark_df \\\n",
    "    .withColumn('offer', F.explode('offer_listing.offers_with_ads'))\n",
    "\n",
    "filtered_df = exploded_df.filter(\n",
    "    (F.col('_PARTITIONTIME') >= F.lit('2024-09-15 00:00:00')) & \n",
    "    (F.col('_PARTITIONTIME') < F.lit('2024-09-16 00:00:00')) &  \n",
    "    (F.col('offer_listing.page') == 1) &\n",
    "    (F.col('search_phrase.raw').isNotNull()) &\n",
    "    (F.col('offer_listing.sorting') == \"RELEVANCE_DESC\") &\n",
    "    (F.col('tag.raw') == 'ProductListing') &\n",
    "    (F.col('offer.position_in_context').isNotNull()) &\n",
    "    (F.col('offer.clicks').isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = filtered_df.withColumn(\n",
    "    'marketplace', F.expr(\"right(service_id, 2)\")\n",
    ").withColumn(\n",
    "    'position', F.col('offer.position_in_context')\n",
    ")\n",
    "\n",
    "aggregated_df = transformed_df.groupBy(\n",
    "    'marketplace', 'position'\n",
    ").agg(\n",
    "    F.count(F.col('offer.clicks')[0]).alias('clicks')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = aggregated_df.orderBy('marketplace', 'position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write calculated data into Big Query dataset\n",
    "NOTE: under the hood data is temporarily stored in *temp_bucket_name* and eventually copied into BQ table, that is why the same location is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.write.format('bigquery') \\\n",
    "  .option('table', bq_table_out) \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data using query\n",
    "NOTE: the dataset is needed for materialization the query results. Connector reads the data from the temp table written into that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('viewsEnabled','true')\n",
    "spark.conf.set('materializationDataset','DATASET')\n",
    "\n",
    "sql = '''\n",
    "  SELECT name, count(*) as cnt from {}\n",
    "  group by name\n",
    "  '''.format(bq_table_in)\n",
    "df = spark.read.format('bigquery').load(sql)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: now you can play with dataframe read that way in normal manner"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m124"
  },
  "kernelspec": {
   "display_name": "Python3.7-Dataproc1.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
